{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE=2000000\n",
    "OUTPUT_FILE = '../raw_data/merged3_data.csv'\n",
    "\n",
    "le_PUZone = LabelEncoder()\n",
    "le_PUBorough = LabelEncoder()\n",
    "scaler1 = MinMaxScaler()\n",
    "scaler2 = StandardScaler()\n",
    "\n",
    "weather_cols = ['prcp', 'snow', 'tavg', 'tmin', 'tmax', 'wspd', 'pres']\n",
    "\n",
    "is_first_chunk = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_68070/1395817498.py:14: DtypeWarning: Columns (3,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv('../raw_data/merged2_data.csv', chunksize=CHUNK_SIZE, parse_dates=['pickup_datetime', 'dropoff_datetime']):\n",
      "/tmp/ipykernel_68070/1395817498.py:14: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for chunk in pd.read_csv('../raw_data/merged2_data.csv', chunksize=CHUNK_SIZE, parse_dates=['pickup_datetime', 'dropoff_datetime']):\n",
      "/tmp/ipykernel_68070/1395817498.py:14: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for chunk in pd.read_csv('../raw_data/merged2_data.csv', chunksize=CHUNK_SIZE, parse_dates=['pickup_datetime', 'dropoff_datetime']):\n",
      "/tmp/ipykernel_68070/1395817498.py:14: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for chunk in pd.read_csv('../raw_data/merged2_data.csv', chunksize=CHUNK_SIZE, parse_dates=['pickup_datetime', 'dropoff_datetime']):\n",
      "/tmp/ipykernel_68070/1395817498.py:14: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for chunk in pd.read_csv('../raw_data/merged2_data.csv', chunksize=CHUNK_SIZE, parse_dates=['pickup_datetime', 'dropoff_datetime']):\n",
      "/tmp/ipykernel_68070/1395817498.py:14: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for chunk in pd.read_csv('../raw_data/merged2_data.csv', chunksize=CHUNK_SIZE, parse_dates=['pickup_datetime', 'dropoff_datetime']):\n",
      "/tmp/ipykernel_68070/1395817498.py:14: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for chunk in pd.read_csv('../raw_data/merged2_data.csv', chunksize=CHUNK_SIZE, parse_dates=['pickup_datetime', 'dropoff_datetime']):\n",
      "/tmp/ipykernel_68070/1395817498.py:14: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for chunk in pd.read_csv('../raw_data/merged2_data.csv', chunksize=CHUNK_SIZE, parse_dates=['pickup_datetime', 'dropoff_datetime']):\n",
      "/tmp/ipykernel_68070/1395817498.py:14: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for chunk in pd.read_csv('../raw_data/merged2_data.csv', chunksize=CHUNK_SIZE, parse_dates=['pickup_datetime', 'dropoff_datetime']):\n",
      "/tmp/ipykernel_68070/1395817498.py:14: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for chunk in pd.read_csv('../raw_data/merged2_data.csv', chunksize=CHUNK_SIZE, parse_dates=['pickup_datetime', 'dropoff_datetime']):\n",
      "/tmp/ipykernel_68070/1395817498.py:14: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for chunk in pd.read_csv('../raw_data/merged2_data.csv', chunksize=CHUNK_SIZE, parse_dates=['pickup_datetime', 'dropoff_datetime']):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Net Stats ===\n",
      "Total rows before cleaning: 29866438\n",
      "Total rows after cleaning: 105\n",
      "Total missing values before cleaning:\n",
      "VendorID                   0\n",
      "pickup_datetime            0\n",
      "dropoff_datetime           0\n",
      "store_and_fwd_flag         0\n",
      "RatecodeID                 0\n",
      "PULocationID               0\n",
      "DOLocationID               0\n",
      "passenger_count            0\n",
      "trip_distance              0\n",
      "fare_amount                0\n",
      "extra                      0\n",
      "mta_tax                    0\n",
      "tip_amount                 0\n",
      "tolls_amount               0\n",
      "improvement_surcharge      0\n",
      "total_amount               0\n",
      "payment_type               0\n",
      "congestion_surcharge       0\n",
      "taxi_type                  0\n",
      "date                       0\n",
      "tavg                     505\n",
      "tmin                     505\n",
      "tmax                     505\n",
      "prcp                     505\n",
      "snow                     505\n",
      "wdir                     505\n",
      "wspd                     505\n",
      "pres                     505\n",
      "location_id                0\n",
      "PUBorough                  0\n",
      "PUZone                     0\n",
      "dtype: int64\n",
      "Total missing values after cleaning:\n",
      "passenger_count          0\n",
      "trip_distance            0\n",
      "fare_amount              0\n",
      "extra                    0\n",
      "mta_tax                  0\n",
      "tip_amount               0\n",
      "tolls_amount             0\n",
      "improvement_surcharge    0\n",
      "total_amount             0\n",
      "payment_type             0\n",
      "congestion_surcharge     0\n",
      "taxi_type                0\n",
      "date                     0\n",
      "tavg                     0\n",
      "tmin                     0\n",
      "tmax                     0\n",
      "prcp                     0\n",
      "snow                     0\n",
      "wdir                     0\n",
      "wspd                     0\n",
      "pres                     0\n",
      "PUBorough                0\n",
      "PUZone                   0\n",
      "trip_duration            0\n",
      "pickup_hour              0\n",
      "pickup_day               0\n",
      "pickup_month             0\n",
      "pickup_year              0\n",
      "pickup_weekend           0\n",
      "temp_variation           0\n",
      "fare_per_mile            0\n",
      "rolling_trip_distance    0\n",
      "rolling_fare_amount      0\n",
      "pickup_hour_sin          0\n",
      "pickup_hour_cos          0\n",
      "pickup_day_sin           0\n",
      "pickup_day_cos           0\n",
      "dtype: int64\n",
      "Pickup Datetime Range: 2002-12-31 23:07:20 to 2098-09-11 02:23:31\n",
      "Dropoff Datetime Range: 2003-01-01 00:38:46 to 2098-09-11 02:52:04\n",
      "\n",
      "Trip Distance Stats (combined across all chunks):\n",
      "count    1.991096e+06\n",
      "mean     1.598095e+02\n",
      "std      7.977861e+01\n",
      "min      9.333333e-01\n",
      "25%      1.048667e+02\n",
      "50%      1.587333e+02\n",
      "75%      2.336000e+02\n",
      "max      1.652413e+04\n",
      "dtype: float64\n",
      "\n",
      "Fare Amount Stats (combined across all chunks):\n",
      "count    1.991096e+06\n",
      "mean     1.556901e+00\n",
      "std      1.183267e+00\n",
      "min     -1.906667e+01\n",
      "25%      1.000000e+00\n",
      "50%      1.066667e+00\n",
      "75%      1.600000e+00\n",
      "max      1.445333e+02\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "total_rows_before = 0\n",
    "total_rows_after = 0\n",
    "missing_values_before = None\n",
    "missing_values_after = None\n",
    "\n",
    "pickup_datetime_min = None\n",
    "pickup_datetime_max = None\n",
    "dropoff_datetime_min = None\n",
    "dropoff_datetime_max = None\n",
    "\n",
    "trip_distance_stats = []\n",
    "fare_amount_stats = []\n",
    "\n",
    "for chunk in pd.read_csv('../raw_data/merged2_data.csv', chunksize=CHUNK_SIZE, parse_dates=['pickup_datetime', 'dropoff_datetime']):\n",
    "    # Update stats before cleaning\n",
    "    total_rows_before += len(chunk)\n",
    "    missing_values_before = chunk.isnull().sum() if missing_values_before is None else missing_values_before + chunk.isnull().sum()\n",
    "\n",
    "    if pickup_datetime_min is None or chunk['pickup_datetime'].min() < pickup_datetime_min:\n",
    "        pickup_datetime_min = chunk['pickup_datetime'].min()\n",
    "    if pickup_datetime_max is None or chunk['pickup_datetime'].max() > pickup_datetime_max:\n",
    "        pickup_datetime_max = chunk['pickup_datetime'].max()\n",
    "\n",
    "    if dropoff_datetime_min is None or chunk['dropoff_datetime'].min() < dropoff_datetime_min:\n",
    "        dropoff_datetime_min = chunk['dropoff_datetime'].min()\n",
    "    if dropoff_datetime_max is None or chunk['dropoff_datetime'].max() > dropoff_datetime_max:\n",
    "        dropoff_datetime_max = chunk['dropoff_datetime'].max()\n",
    "\n",
    "    trip_distance_stats.append(chunk['trip_distance'].describe())\n",
    "    fare_amount_stats.append(chunk['fare_amount'].describe())\n",
    "\n",
    "    # Apply preprocessing steps here\n",
    "    chunk = chunk.dropna()\n",
    "    chunk = chunk.drop_duplicates()\n",
    "    chunk.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    chunk['pickup_datetime'] = pd.to_datetime(chunk['pickup_datetime'], errors='coerce')\n",
    "    chunk['dropoff_datetime'] = pd.to_datetime(chunk['dropoff_datetime'], errors='coerce')\n",
    "    chunk = chunk.dropna(subset=['pickup_datetime', 'dropoff_datetime'])\n",
    "\n",
    "    chunk['trip_duration'] = (chunk['dropoff_datetime'] - chunk['pickup_datetime']).dt.total_seconds() / 60\n",
    "    chunk = chunk[(chunk['trip_duration'] > 1) & (chunk['trip_duration'] <= 120)]  # Remove unrealistic durations\n",
    "\n",
    "    # Temporal Features\n",
    "    chunk['pickup_hour'] = chunk['pickup_datetime'].dt.hour\n",
    "    chunk['pickup_day'] = chunk['pickup_datetime'].dt.dayofweek\n",
    "    chunk['pickup_month'] = chunk['pickup_datetime'].dt.month\n",
    "    chunk['pickup_year'] = chunk['pickup_datetime'].dt.year\n",
    "\n",
    "    # Keep only records with dates in 2021\n",
    "    chunk = chunk[(chunk['pickup_datetime'].dt.year == 2021) & (chunk['dropoff_datetime'].dt.year == 2021)]\n",
    "    chunk['pickup_weekend'] = chunk['pickup_day'].isin([5, 6]).astype(int)\n",
    "\n",
    "    chunk[weather_cols] = scaler1.fit_transform(chunk[weather_cols])\n",
    "    chunk['temp_variation'] = chunk['tmax'] - chunk['tmin']\n",
    "    chunk[['trip_distance', 'fare_amount', 'tmin', 'tmax', 'prcp', 'snow']] = scaler2.fit_transform(chunk[['trip_distance', 'fare_amount', 'tmin', 'tmax', 'prcp', 'snow']])\n",
    "\n",
    "    columns_to_drop = ['VendorID', 'PULocationID', 'DOLocationID', 'store_and_fwd_flag', 'RatecodeID','location_id', 'pickup_datetime', 'dropoff_datetime']\n",
    "    chunk.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "    if is_first_chunk:  # Fit encoders only for the first chunk\n",
    "        le_PUZone.fit(chunk['PUZone'])\n",
    "        le_PUBorough.fit(chunk['PUBorough'])\n",
    "\n",
    "    chunk['PUZone'] = le_PUZone.transform(chunk['PUZone'])\n",
    "    chunk['PUBorough'] = le_PUBorough.transform(chunk['PUBorough'])\n",
    "\n",
    "    # Encoding taxi_type (yellow = 0, green = 1)\n",
    "    chunk['taxi_type'] = chunk['taxi_type'].map({'yellow': 0, 'green': 1})\n",
    "\n",
    "    # Encoding payment_type (map integer values to descriptive categories, e.g., 1 = Cash, 2 = Card, 3 = Mobile Payment)\n",
    "    payment_map = {1: 'Cash', 2: 'Card', 3: 'Mobile Payment'}\n",
    "    chunk['payment_type'] = chunk['payment_type'].map(payment_map)\n",
    "\n",
    "    chunk['fare_per_mile'] = chunk['fare_amount'] / chunk['trip_distance']\n",
    "\n",
    "    # Rolling and cyclic features\n",
    "    chunk['rolling_trip_distance'] = chunk['trip_distance'].rolling(window=3).mean()\n",
    "    chunk['rolling_fare_amount'] = chunk['fare_amount'].rolling(window=3).mean()\n",
    "    chunk['pickup_hour_sin'] = np.sin(2 * np.pi * chunk['pickup_hour'] / 24)\n",
    "    chunk['pickup_hour_cos'] = np.cos(2 * np.pi * chunk['pickup_hour'] / 24)\n",
    "    chunk['pickup_day_sin'] = np.sin(2 * np.pi * chunk['pickup_day'] / 7)\n",
    "    chunk['pickup_day_cos'] = np.cos(2 * np.pi * chunk['pickup_day'] / 7)\n",
    "\n",
    "    chunk = chunk.dropna()\n",
    "\n",
    "    chunk = chunk[(chunk['trip_distance'] > 0) & (chunk['trip_distance'] <= 50)]  # Remove extreme distances\n",
    "    chunk = chunk[(chunk['fare_amount'] > 0) & (chunk['fare_amount'] <= 500)]  # Remove extreme fares\n",
    "\n",
    "    # Write Processed Chunk to CSV\n",
    "    if is_first_chunk:\n",
    "        chunk.to_csv(OUTPUT_FILE, index=False, mode='w')  # Write with headers for the first chunk\n",
    "        is_first_chunk = False\n",
    "    else:\n",
    "        chunk.to_csv(OUTPUT_FILE, index=False, header=False, mode='a')  # Append without headers\n",
    "\n",
    "\n",
    "    # Update stats after cleaning\n",
    "    total_rows_after += len(chunk)\n",
    "    missing_values_after = chunk.isnull().sum() if missing_values_after is None else missing_values_after + chunk.isnull().sum()\n",
    "\n",
    "# Compute final stats\n",
    "print(\"\\n=== Final Net Stats ===\")\n",
    "print(\"Total rows before cleaning:\", total_rows_before)\n",
    "print(\"Total rows after cleaning:\", total_rows_after)\n",
    "print(\"Total missing values before cleaning:\")\n",
    "print(missing_values_before)\n",
    "print(\"Total missing values after cleaning:\")\n",
    "print(missing_values_after)\n",
    "print(\"Pickup Datetime Range:\", pickup_datetime_min, \"to\", pickup_datetime_max)\n",
    "print(\"Dropoff Datetime Range:\", dropoff_datetime_min, \"to\", dropoff_datetime_max)\n",
    "\n",
    "# Combine trip distance and fare stats across chunks\n",
    "combined_trip_distance_stats = pd.concat(trip_distance_stats, axis=1).mean(axis=1)\n",
    "combined_fare_amount_stats = pd.concat(fare_amount_stats, axis=1).mean(axis=1)\n",
    "\n",
    "print(\"\\nTrip Distance Stats (combined across all chunks):\")\n",
    "print(combined_trip_distance_stats)\n",
    "print(\"\\nFare Amount Stats (combined across all chunks):\")\n",
    "print(combined_fare_amount_stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
