{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"../raw_data\"\n",
    "\n",
    "yellow_data_path = os.path.join(data_folder, \"yellow_data.csv\")\n",
    "green_data_path = os.path.join(data_folder, \"green_data.csv\")\n",
    "preprocessed_data_path = os.path.join(data_folder, \"preprocessed_taxi_data.csv\")\n",
    "weather_data_path = os.path.join(data_folder, \"weather_data.json\")\n",
    "zone_data_path = os.path.join(data_folder, \"taxi_zones.csv\")\n",
    "merged_data_path = os.path.join(data_folder, \"merged_data.csv\")\n",
    "merged2_data_path = os.path.join(data_folder, \"merged2_data.csv\")\n",
    "merged3_data_path = os.path.join(data_folder, \"merged3_data.csv\")\n",
    "\n",
    "CHUNK_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_chunk(chunk, taxi_type, is_green=False):\n",
    "    chunk['taxi_type'] = taxi_type\n",
    "    # Standardize column names\n",
    "    chunk.rename(columns={\n",
    "        'tpep_pickup_datetime': 'pickup_datetime',\n",
    "        'tpep_dropoff_datetime': 'dropoff_datetime',\n",
    "        'lpep_pickup_datetime': 'pickup_datetime',\n",
    "        'lpep_dropoff_datetime': 'dropoff_datetime',\n",
    "    }, inplace=True)\n",
    "    # Drop unnecessary columns for green taxis\n",
    "    if is_green:\n",
    "        chunk.drop(columns=['trip_type'], inplace=True)\n",
    "        chunk.drop(columns=['ehail_fee'], inplace=True)\n",
    "    chunk.dropna(inplace=True)\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_first_chunk = True\n",
    "\n",
    "# Process Green Taxi Data in Chunks\n",
    "print(\"Processing Green Taxi Data...\")\n",
    "for green_chunk in pd.read_csv(green_data_path, chunksize=CHUNK_SIZE, low_memory=False):\n",
    "    green_chunk = preprocess_chunk(green_chunk, taxi_type='green', is_green=True)\n",
    "    green_chunk.to_csv(preprocessed_data_path, mode='w' if is_first_chunk else 'a', header=is_first_chunk, index=False)\n",
    "    is_first_chunk = False\n",
    "\n",
    "# Process Yellow Taxi Data in Chunks\n",
    "print(\"Processing Yellow Taxi Data...\")\n",
    "for yellow_chunk in pd.read_csv(yellow_data_path, chunksize=CHUNK_SIZE, low_memory=False):\n",
    "    yellow_chunk = preprocess_chunk(yellow_chunk, taxi_type='yellow')\n",
    "    yellow_chunk.to_csv(preprocessed_data_path, mode='w' if is_first_chunk else 'a', header=is_first_chunk, index=False)\n",
    "    is_first_chunk = False\n",
    "\n",
    "print(f\"Final preprocessed data saved to '{preprocessed_data_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          date  tavg  tmin  tmax  prcp  snow   wdir  wspd    pres\n",
      "0   2021-01-01   3.6   2.2   4.4  17.3   0.0   36.0  13.2  1029.4\n",
      "1   2021-01-02   6.3   3.3  12.2   6.1   0.0  319.0  11.6  1012.9\n",
      "2   2021-01-03   3.4   2.8   4.4   5.7   0.0   33.0  22.4  1017.1\n",
      "3   2021-01-04   4.6   2.8   7.2   0.7   0.0    7.0   8.1  1015.0\n",
      "4   2021-01-05   4.4   3.3   5.6   0.0   0.0    0.0   8.1  1013.5\n",
      "..         ...   ...   ...   ...   ...   ...    ...   ...     ...\n",
      "360 2021-12-27   1.4  -0.6   4.1   5.3   0.0   33.0   8.9  1017.2\n",
      "361 2021-12-28   6.1   3.7   9.0   3.4   0.0  297.0   8.0  1010.3\n",
      "362 2021-12-29   6.3   4.4   9.9   7.7   0.0   49.0   9.4  1012.0\n",
      "363 2021-12-30   6.4   4.1   9.9   1.7   0.0   33.0   5.7  1013.9\n",
      "364 2021-12-31   8.2   5.7  10.1   0.3   0.0  218.0   8.0  1013.9\n",
      "\n",
      "[365 rows x 9 columns]\n",
      "     location_id        borough                       zone\n",
      "0              1            EWR             Newark Airport\n",
      "1              2         Queens                Jamaica Bay\n",
      "2              3          Bronx    Allerton/Pelham Gardens\n",
      "3              4      Manhattan              Alphabet City\n",
      "4              5  Staten Island              Arden Heights\n",
      "..           ...            ...                        ...\n",
      "258          256       Brooklyn  Williamsburg (South Side)\n",
      "259          259          Bronx         Woodlawn/Wakefield\n",
      "260          260         Queens                   Woodside\n",
      "261          261      Manhattan         World Trade Center\n",
      "262          262      Manhattan             Yorkville East\n",
      "\n",
      "[260 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "with open(weather_data_path, 'r') as f:\n",
    "    weather_json = json.load(f)\n",
    "weather_data = weather_json[\"data\"]\n",
    "weather_df = pd.DataFrame(weather_data)\n",
    "weather_df = weather_df.drop_duplicates()\n",
    "weather_df = weather_df.drop(columns=['tsun', 'wpgt'])\n",
    "\n",
    "zone_df = pd.read_csv(zone_data_path)\n",
    "zone_df = zone_df.drop_duplicates()\n",
    "\n",
    "# Ensure weather data date format is correct and convert to datetime object\n",
    "weather_df['date'] = pd.to_datetime(weather_df['date'], format='%Y-%m-%d', errors='coerce')\n",
    "\n",
    "# Preview date formats\n",
    "print(weather_df)\n",
    "print(zone_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Merging taxi data with weather data...\")\n",
    "\n",
    "for chunk in pd.read_csv(\n",
    "    preprocessed_data_path,\n",
    "    chunksize=CHUNK_SIZE,\n",
    "    parse_dates=['dropoff_datetime']):\n",
    "    # Extract the date from dropoff_datetime and convert to datetime\n",
    "    chunk['date'] = chunk['dropoff_datetime'].dt.normalize()  # Extract the date part\n",
    "\n",
    "    # Ensure chunk['date'] is properly converted to datetime\n",
    "    chunk['date'] = pd.to_datetime(chunk['date'], errors='coerce')\n",
    "\n",
    "    # Ensure weather_df['date'] is also datetime before merging\n",
    "    weather_df['date'] = pd.to_datetime(weather_df['date'], errors='coerce').dt.normalize()\n",
    "\n",
    "    # Debugging: print first few values and types to check conversion\n",
    "    print(f\"chunk['date'] preview: {chunk['date'].head()}\")\n",
    "\n",
    "    # Merge the chunk with weather data on the 'date' column\n",
    "    merged_chunk = chunk.merge(weather_df, how='left', on='date')\n",
    "\n",
    "    # Save merged data incrementally\n",
    "    merged_chunk.to_csv(merged_data_path, mode='w' if is_first_chunk else 'a', header=is_first_chunk, index=False)\n",
    "    is_first_chunk = False\n",
    "\n",
    "print(f\"Merged data saved to '{merged_data_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m n2_total_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Read the data in chunks\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(merged_data_path, chunksize\u001b[38;5;241m=\u001b[39mCHUNK_SIZE):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Count the occurrences of 'N' in DOLocationID for the current chunk\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     n_count \u001b[38;5;241m=\u001b[39m (chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDOLocationID\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m      8\u001b[0m     n_count2 \u001b[38;5;241m=\u001b[39m (chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPULocationID\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "CHUNK_SIZE = 10000\n",
    "n_total_count = 0  # Variable to accumulate the count of 'N'\n",
    "n2_total_count = 0\n",
    "# Read the data in chunks\n",
    "for chunk in pd.read_csv(merged_data_path, chunksize=CHUNK_SIZE):\n",
    "    # Count the occurrences of 'N' in DOLocationID for the current chunk\n",
    "    n_count = (chunk['DOLocationID'] == 'N').sum()\n",
    "    n_count2 = (chunk['PULocationID'] == 'N').sum()\n",
    "\n",
    "    # Accumulate the count\n",
    "    n_total_count += n_count\n",
    "    n2_total_count += n_count2\n",
    "\n",
    "    # Debugging: Print the count for the current chunk\n",
    "    print(f\"Count of 'N' in DOLocationID for this chunk: {n_count}\")\n",
    "    print(f\"Count of 'N2' in PULocationID for this chunk: {n_count2}\")\n",
    "\n",
    "# After processing all chunks, print the total count of 'N' in DOLocationID\n",
    "print(f\"Total count of 'N' in DOLocationID: {n_total_count}\")\n",
    "print(f\"Total count of 'N2' in PULocationID: {n2_total_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged 2 data saved to '../raw_data/merged2_data.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "processed_trip_data = pd.DataFrame()\n",
    "is_first_chunk = True\n",
    "\n",
    "CHUNK_SIZE = 10000\n",
    "\n",
    "for chunk in pd.read_csv(merged_data_path, chunksize=CHUNK_SIZE):\n",
    "\n",
    "    chunk['PULocationID'] = chunk['PULocationID'].astype(str).str.strip()\n",
    "    zone_df['location_id'] = zone_df['location_id'].astype(str).str.strip()\n",
    "\n",
    "    chunk = chunk.merge(zone_df, how='left', left_on='PULocationID', right_on='location_id')\n",
    "    chunk.rename(columns={'borough': 'PUBorough', 'zone': 'PUZone'}, inplace=True)\n",
    "\n",
    "    chunk.dropna(subset=['PUBorough'], inplace=True)\n",
    "    chunk.to_csv(merged2_data_path, mode='a', header=is_first_chunk, index=False)\n",
    "    is_first_chunk = False\n",
    "\n",
    "print(f\"Merged 2 data saved to '{merged2_data_path}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
